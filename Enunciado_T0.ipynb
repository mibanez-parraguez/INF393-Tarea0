{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Máquinas de Aprendizaje </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Introducción a librerías comunes de *Machine Learning*:\n",
    "    * Pandas\n",
    "    * Numpy\n",
    "    * Sklearn\n",
    "    * Matplotlib,..\n",
    "* Clasificación y regresión sobre texto.\n",
    "* Implementación a mano de regresión lineal.\n",
    "* Algoritmo Gradiente descendente\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega y discusión: 5 de Octubre.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF393-II-2018]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Sentiment Analysis  \n",
    "[2.](#segundo) Job Salary Prediction  \n",
    "[3.](#tercero) Linear Regression by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Sentiment Analysis [Sin evaluación]\n",
    "El análisis de emociones o sentimientos se refiere al proceso de extraer información acerca de la actitud\n",
    "que una persona (o grupo de ellas) manifiesta, en un determinado medio o formato digital, con respecto a un\n",
    "tópico o contexto de comunicación. Uno de los casos más estudiados corresponde a determinar la polaridad\n",
    "de un trozo de texto, es decir, clasificar una determinada evaluación escrita (*review*), en que una persona\n",
    "manifiesta una opinión, como *positiva*, *negativa* o *neutral*. Esto también ha sido extendido a otros medios, como lo es analizar la polaridad de textos en redes sociales.  \n",
    "\n",
    "<img src=\"https://cdn.urgente24.com/sites/default/files/notas/2018/01/18/twitter.png\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "La conocida red social Twitter tiene una gran cantidad de usuarios, por lo que la información se genera a\n",
    "cada segundo, donde el análisis de texto se ha aplicado fuertemente a estos medios sociales. La dificultad de\n",
    "este problema radica en el carácter altamente ambiguo e informal del lenguaje que utilizan naturalmente las\n",
    "personas ası́ como el manejo de negaciones, sarcasmo y abreviaciones en una frase.\n",
    "\n",
    "\n",
    "Los datos pueden ser descargados ejecutando el siguiente código en sistema Unix:\n",
    "```\n",
    "wget -O train_data.csv http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\n",
    "wget -O test_data.csv http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\n",
    "```\n",
    "\n",
    "Para construir un clasificador que determine automáticamente la polaridad de un trozo de texto, vamos a necesitar representar los textos $\\{d_i\\}_{i=1}^n$ disponibles como vectores de caracterı́sticas (features). Para aumentar la eficacia de las caracterı́sticas extraı́das es conveniente ejecutar algunas técnicas de pre-procesamiento básicas como: pasar todo el texto a minúsculas (lower-casing), eliminar signos de puntuación\n",
    "y eliminar palabras sin significado como artı́culos, pronombres y preposiciones (*stop word removal*). Otra\n",
    "técnica que suele ser útil para obtener buenas caracterı́sticas (*features*) es la lematización, es decir, la\n",
    "reducción de todas las palabras a su tronco léxico base. Una técnica similar y más utilizada en la práctica es el stemming. Varias de éstas están implementadas en la libreria nltk [13] para python.\n",
    "\n",
    "\n",
    "> a) Construya un dataframe con los datos a analizar. Determine cuántas clases existen, cuántos registros\n",
    "por clase y describa el dataset.\n",
    "``` python\n",
    "import pandas as pd\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "df_train = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "df_train['Sentiment'] = pd.to_numeric(df_train['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "df_test = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "df_test['Sentiment'] = pd.to_numeric(df_test['Sentiment'])\n",
    "df_train.shape\n",
    "```\n",
    "\n",
    "> b) Construya un conjunto de validación (también conocido como *hold out validation*), a través de una máscara aleatoria, para verificar los resultados de los algoritmos.\n",
    "``` python\n",
    "import numpy as np\n",
    "msk = np.random.rand(len(df_train)) < 0.8\n",
    "df_new_train = df_train[msk]\n",
    "df_val = df_train[~msk]\n",
    "```\n",
    "\n",
    "> c) Implemente y explique un pre-procesamiento para los tweets para dejarlos en un formato estándarizado\n",
    "en el cual se podrán trabajar.\n",
    "```python\n",
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def word_extractor(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    wordstemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordstemmer.stem(word.lower())for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "word_extractor(\"I love to eat cake\")\n",
    "word_extractor(\"I love eating cake\")\n",
    "word_extractor(\"I loved eating the cake\")\n",
    "word_extractor(\"I do not love eating cake\")\n",
    "word_extractor(\"I don't love eating cake\")\n",
    "...\n",
    "wordlemmatizer.lemmatize(word.lower()) #can use this\n",
    "```\n",
    "\n",
    "> d) Utilizando la función *CountVectorizer* de la librerı́a sklearn genere una representación vectorial del texto de entrenamiento y del conjunto que usaremos para realizar la validación. Esta función consiste en contar cuántas veces aparecen ciertos términos/palabras en el texto a través de un vocabulario que construiremos mediante la unión de todas las palabras que observemos en los textos que tenemos a disposición. Explore el vocabulario utilizado y determine cuáles son las palabras más frecuentes en el conjunto de entrenamiento y validación.\n",
    "``` python\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts_train = [word_extractor(text) for text in df_new_train.Text]\n",
    "texts_val = [word_extractor(text) for text in df_val.Text]\n",
    "texts_test = [word_extractor(text) for text in df_test.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_val = vectorizer.transform(texts_val)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((df_new_train.Sentiment.astype(float)+1)/2.0)\n",
    "labels_val = np.asarray((df_val.Sentiment.astype(float)+1)/2.0)\n",
    "labels_test = np.asarray((df_test.Sentiment.astype(float)+1)/2.0)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag\n",
    "```\n",
    "\n",
    "> e) Construya una función que entrene/ajuste un modelo de Regresión Logı́stica y mida el error de predicción obtenido sobre los datos de entrenamiento y validación. Experimente con variar la representación o preprocesamiento, por ejemplo el efecto de filtrar *stopwords* y de eliminar este paso de\n",
    "pre-procesamiento tı́pico. Determine además, qué representación obtiene un mejor resultado: si aquella obtenida vı́a lematización o aquella obtenida vı́a *stemming*. Seleccione el mejor modelo y mida sobre *test*. Comente\n",
    "``` python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def do_LOGIT(x,y,xv,yv):\n",
    "    model = LogisticRegression()\n",
    "    model = model.fit(x, y)\n",
    "    print(\"Accuracy under training: \",model.score(x,y))\n",
    "    print(\"Accuracy under validation: \",model.score(xv,yv))\n",
    "    return model\n",
    "log_model = do_LOGIT(features_train,labels_train,features_val,labels_val)\n",
    "```\n",
    "\n",
    "> f) Finalmente, tome un subconjunto aleatorio de los textos de prueba y analice las predicciones del modelo (explore las predicciones, ası́ como las probabilidades que el clasificador asigna a cada clase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Job Salary Prediction\n",
    "\n",
    "En esta sección se trabajará con el problema de predecir el salario que ofrece un anuncio en internet a través unicamente del texto del anuncio. El dataset es ofrecido por *Adzuna* como una competencia en la plataforma más grande de *data science* Kaggle, a través del siguiente __[link](https://www.kaggle.com/c/job-salary-prediction)__. El objetivo de la competencia, según *Adzuna*, es el de tener un motor que pueda predecir el salario de cualquier anuncio de trabajo en Reunio Unido, para poder mejorar la experiencia de los usuarios que buscan trabajos, ya que así pueden filtrar sin que el mismo empleador señale explícitamente cuánto paga.\n",
    "\n",
    "<img src=\"http://s5047.pcdn.co/wp-content/uploads/2013/05/salary-prediction-engine-v2.png\" title=\"Title text\" width=\"50%\"/>\n",
    "\n",
    "La métrica de evaluación de la competencia es MAE (*mean absolute error*):\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_i^N  \\mid y_i - \\hat{y}_i \\mid\n",
    "$$\n",
    "\n",
    "\n",
    "Para descargar los datos a utilizar (*Train_rev1*) debe estar registrado en la plataforma de Kaggle. Se cuenta con cientos de miles registros con textos sin procesar, es decir, no están estructurados.\n",
    "\n",
    "\n",
    "> a) Carge los datos *csv* de entrenamiento y cree un conjunto de validación con los últimos 10 mil datos en un dataframe de *pandas*. Describa los datos, apóyese de gráficos ¿Cuántos datos hay en cada conjunto?  \n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Train_rev1.csv\")\n",
    "df_train = df.iloc[:-10000]\n",
    "df_val = df.iloc[-10000:]\n",
    "...#load other sets\n",
    "df.head()\n",
    "```\n",
    "*Recuerde que si no puede trabajar con todos los datos, debido a su volumen, puede muestrear*.\n",
    "\n",
    "\n",
    "> b) Extraiga los datos de cada conjunto con los que trabajará, el *input* $X$, los textos, y el *output* $y$, los salarios.\n",
    "```python\n",
    "text = df.FullDescription\n",
    "salary = df.SalaryNormalized\n",
    "```\n",
    "\n",
    "> c) Realice un pre-procesamiento a los datos brutos de texto para extraer características y generar la representación de los datos de entrada al modelo $\\vec{x}$. Comente sobre lo realizado.\n",
    "\n",
    "> d) Intente resolver el problema enfrentándolo como regresión con el modelo de regresión lineal ordinaria en *sklearn*. ¿Qué es lo que hace *fit_intercept=True*? Evalúe la función objetivo (F.O.) utilizada y la métrica de la competencia (*mean absolute error*) en ambos conjuntos generados en el punto a). Comente lo observado.\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "model = LR(fit_intercept=True, normalize=False)\n",
    "model.fit(X_train,y_train)\n",
    "... #measure F.O.\n",
    "from sklearn.metrics import mean_absolute_error #measure MAE\n",
    "print(\"MAE on train: \",mean_absolute_error(y_train, model.predict(X_train)))\n",
    "print(\"MAE on validation: \",mean_absolute_error(y_val, model.predict(X_val)))\n",
    "```\n",
    "\n",
    "> e) Intente mejorar sus resultados en base a la métrica de la competencia (MAE) sobre el conjunto de validación. Comente sobre lo realizado.  \n",
    "*No se le pide que imite los resultados ganadores de la competencia (MAE de 3400), sino que mejore lo ya alcanzado siendo creativo.*\n",
    "<div class=\"alert alert-warning\"> HINT: Una opción es cambiar el *approach* de resolución desde regresión a casificación o trabajo sobre los datos (tal como limpiarlos).</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Linear Regression by hand\n",
    "\n",
    "En esta sección se le pedirá que implemente la regresión lineal ordinaria a través del algoritmo SGD (*Stochastic Gradient Descend*) para encontrar los parámetros de la regresión a través de este algoritmo de manera iterativa. La técnica de SGD es sin duda dominante al momento de entrenar modelos en máquinas de aprendizaje cuando la solución no tiene un óptimo derivable analíticamente, en este caso la regresión lineal ordinaria que trabajaremos si tiene óptimo anaĺitico, sin embargo, se le pedirá que compare este caso con fines pedagógicos.\n",
    "\n",
    "* Regresión lineal ordinaria:\n",
    "$$\n",
    "\\hat{y} = f(\\vec{x}) =\\vec{\\beta}^T\\cdot \\vec{x}\n",
    "$$\n",
    "\n",
    "* Función objetivo:\n",
    "$$\n",
    "Loss = \\frac{1}{N} \\sum_i^N ( y_i - \\hat{y}_i )^2\n",
    "$$\n",
    "\n",
    "\n",
    "* Algoritmo SGD para regresión lineal ordinaria:\n",
    "$$ \\vec{\\beta}^{(t+1)} \\leftarrow \\vec{\\beta}^{(t)} - \\eta \\nabla_{\\vec{\\beta}^{(t)}} Loss $$\n",
    "\n",
    "Para lo que sigue de la actividad sólo podrá utilizar *numpy* (para operaciones de algebra lineal).\n",
    "\n",
    "> a) Escriba una función que calcule la función de pérdida, error cuadrático medio (MSE - *mean squared error*), para un dato o para un conjunto de datos.\n",
    "\n",
    "> b) Escriba una función que calcule el gradiente (derivada) de la función de pérdida anterior, para un dato o para un conjunto de datos. *Escriba explícitamente la derivada (gradiente)*.\n",
    "\n",
    "> c) Escriba una función que calcule los parámetros de una regresión lineal simple de manera analítica (es decir el mínimo global). \n",
    "\n",
    "<div class=\"alert alert-warning\"> Hint: ésto debería ser la ecuación desglosada de la derivada igual a 0, la cual incluye $(X^TX)^{-1}$</div>\n",
    "\n",
    "\n",
    "> d) Ahora escriba un programa que permita entrenar una regresión lineal a través del algoritmo SGD mostrado en la ecuación del algoritmo SGD, es decir, que de manera iterativa, vaya tomando un dato a la vez, y actualizando el parámetro $\\beta$ a través del gradiente descendiente de la función de pérdida de la regresión lineal ordinaria, de la pregunta b).\n",
    "\n",
    "> e) Demuestre que sus programas funcionan en un problema de regresión simple. Para esto utilice el dataset **Boston Housing** , disponible a través de la librería __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a el precio de diferentes casas en Boston además de distintas características relevantes respecto al lugar, como por ejemplo el crimen en la ciudad, el número de habitaciones, que tan vieja es, distancia a lugares relevantes, entre otros. Éstas características deben combinarse linealmente para estimar el precio de la casa.\n",
    "<div class=\"alert alert-block alert-info\">Es una buena práctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_boston\n",
    "X_train,y_train = load_boston(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "```\n",
    "Para evaluar los resultados, construya un gráfico correspondiente a la función de pérdida utilizada en el entrenamiento *versus* número de iteraciones (**realice 1000 iteraciones**), utilizando sólo el conjunto de entrenamiento (el objetivo de esta sección es familiarizarse con el algoritmo). Además de reportar el tiempo de entrenamiento mediante el algoritmo implementado en c) y d).\n",
    "\n",
    "> e) Varié la tasa de aprendizaje $\\eta \\in [0,1]$ del algoritmo SGD del punto d), compare los resultados entre sí y con la solución óptima encontrada en c). Comente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "[1] Hastie, T.; Tibshirani, R., Friedman, J. (2009), *The Elements of Statistical Learning*, Second Edition.\n",
    "Springer New York Inc.  \n",
    "[2] http://statweb.stanford.edu/tibs/ElemStatLearn/datasets/  \n",
    "[3] https://en.wikipedia.org/wiki/Stop words  \n",
    "[4] https://en.wikipedia.org/wiki/Stemming  \n",
    "[5] https://en.wikipedia.org/wiki/Lemmatisation  \n",
    "[6] http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature extraction.text  \n",
    "[7] http://www.nltk.org/  \n",
    "[8] https://gluon.mxnet.io/chapter06_optimization/gd-sgd-scratch.html  \n",
    "[9] https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
